{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoder from training data\n",
    "* Here is about how we are going to process data based on the information in train dataset. It can be encoders for the text and categorical variables or mean and standard deviation for scaling numeric variables or imputation values for the missing values. \n",
    "* The output from this step will be used to transform data into the format that the model can train or predict on.\n",
    "\n",
    "#### Input \n",
    "* train.parquet\n",
    "\n",
    "#### Output\n",
    "* preprocessor.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"mean\" # or \"median\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import cloudpickle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "\n",
    "# file and directory info\n",
    "project_dir = os.path.dirname(os.getcwd())\n",
    "interim_folder = \"/data/interim/\"\n",
    "processed_folder = \"/data/processed/\"\n",
    "var_y = 'quality'\n",
    "\n",
    "train = pd.read_parquet(project_dir + interim_folder + 'train.parquet')\n",
    "var_numeric_x = train.columns.tolist()\n",
    "var_numeric_x.remove(var_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_X = Pipeline(steps=[\n",
    "    ('x_imputer', SimpleImputer(strategy = method)),\n",
    "    ('x_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor_Y = Pipeline(steps=[\n",
    "    ('y_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "train_X_untransformed = train.drop([var_y], axis=1)\n",
    "train_Y_untransformed = train[var_y].to_frame()\n",
    "\n",
    "preprocessor_X.fit(train_X_untransformed)\n",
    "preprocessor_Y.fit(train_Y_untransformed)\n",
    "\n",
    "with open(project_dir + processed_folder + 'preprocessor_X.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(preprocessor_X, f)\n",
    "with open(project_dir + processed_folder + 'preprocessor_Y.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(preprocessor_Y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('x_imputer',\n                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n                               missing_values=nan, strategy='mean',\n                               verbose=0)),\n                ('x_scaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True))],\n         verbose=False)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('y_scaler', MinMaxScaler(copy=True, feature_range=(0, 1)))],\n         verbose=False)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor_Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3 (template_python)",
   "language": "R",
   "name": "template_python"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}